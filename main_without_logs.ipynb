{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_without_logs.ipynb","private_outputs":true,"provenance":[{"file_id":"1g_HEOTTuWpbakfRo8Ma2hKMfzHv0aq9I","timestamp":1616364657708}],"collapsed_sections":[],"authorship_tag":"ABX9TyN+YqCwtP16azjLZlGuIOB9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AX0E-RbRirbx"},"source":["import sys\n","import importlib\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","\n","%matplotlib inline\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZQEvNk_4W2-"},"source":["# link to project modules\n","sys.path.insert(0,'/content/drive/MyDrive/Projects/UncertaintyExp')\n","\n","# import custom modules\n","import utils, pipelines, models, train, predict, results\n","\n","# reload all modules (google colab doesnt automatically detect \n","# updated custom modules unless reloading)\n","importlib.reload(utils)\n","importlib.reload(pipelines)\n","importlib.reload(models)\n","importlib.reload(train)\n","importlib.reload(predict)\n","importlib.reload(results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gyKORiX1G4J"},"source":["# general setttings\n","seed = 1\n","n_experiments = 10\n","batch_size = 128\n","epochs = 200\n","train_split = 0.8\n","n_hidden = 100\n","tau = 0.15\n","test_iters = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4QdfcWgSWf1c"},"source":["# Select dataset and index of target\n","dataset, label_index = 'bostonHousing', 13\n","\n","# reload models module\n","importlib.reload(models)\n","\n","# instantiate final logs\n","final_logs = []\n","\n","# set gpu\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# start experiments\n","for experiment in range(n_experiments):\n","  experiment_logs = []\n","  seed = experiment\n","  # get data\n","  train, val, test, target_scale = pipelines.from_git(dataset=dataset, \n","                                  label_index=label_index,\n","                                  split=train_split, seed=seed, device=device)\n","  n_features=train[0].shape[1]\n","  n_train = train[0].shape[0]\n","  steps = np.int(n_train/batch_size)+1\n","  target_scale = target_scale\n","  print(f\"Experiment {experiment+1}: train sample: {n_train}, batchsize: {batch_size}, steps: {steps}\")\n","  # instantiate models list\n","  model_list = [models.Baseline, models.MCDropout, models.SDENet, models.IVNet]   \n","  for model in model_list:\n","    # compile models\n","    net = model(n_features=n_features, n_hidden=n_hidden, tau=tau, n_train=n_train, layer_depth=6, device=device).to(device)\n","    train_optim, train_sched = net.custom_compile()\n","    # instantiate model logs\n","    train_logs = []\n","    val_logs_1, val_logs_2 = [], []\n","    test_logs_1, test_logs_2 = [], []\n","    # start training\n","    for epoch in range(epochs):\n","      permutation = utils.shuffle(train) # not needed \n","      # start epoch\n","      for step in range(steps):\n","        batch = utils.load_batch(step, train, batch_size, permutation, shuffle_epoch=False)\n","        # get train logs\n","        train_log = net.train_step(model=net, batch=batch, optim=train_optim,\n","                                     scheduler=train_sched, train_logs=train_logs, batch_size=batch_size)\n","        train_logs.append(train_log)\n","      # get validation logs\n","      val_logs = net.evaluation_step(model=net,test_tuple=val, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      # get test logs\n","      test_logs = net.evaluation_step(model=net,test_tuple=test, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      val_logs_1.append(val_logs[0]), val_logs_2.append(val_logs[1])      \n","      test_logs_1.append(test_logs[0]), test_logs_2.append(test_logs[1])\n","    # only store test logs at mininum validation loss\n","    experiment_logs.append([test_logs_1[np.argmin(val_logs_1)], test_logs_2[np.argmin(val_logs_1)]])\n","    # log plots\n","    utils.log_plots(model_name=net.custom_name, dataset_name=dataset, \n","                    train_logs = train_logs, val_logs_1 = val_logs_1,\n","                    val_logs_2 = val_logs_2, test_logs_1 = test_logs_1,\n","                     test_logs_2 = test_logs_2, remove_n =0, log_transform=False)       \n","    \n","  final_logs.append(experiment_logs)\n","# show results\n","results.resultdf(model_list, final_logs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xiRqT15WXRiY"},"source":["dataset, label_index = 'wine-quality-red', 11\n","importlib.reload(models)\n","final_logs = []\n","best_log = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for experiment in range(n_experiments):\n","  experiment_logs = []\n","  seed = experiment\n","  train, val, test, target_scale = pipelines.from_git(dataset=dataset, \n","                                  label_index=label_index,\n","                                  split=train_split, seed=seed, device=device)\n","  n_features=train[0].shape[1]\n","  n_train = train[0].shape[0]\n","  steps = np.int(n_train/batch_size)+1\n","  target_scale = target_scale\n","  print(f\"Experiment {experiment+1}: train sample: {n_train}, batchsize: {batch_size}, steps: {steps}\")\n","  model_list = [models.Baseline, models.MCDropout, models.SDENet, models.IVNet]   \n","  for model in model_list:\n","    net = model(n_features=n_features, n_hidden=n_hidden, tau=tau, n_train=n_train, layer_depth=6, device=device).to(device)\n","    train_optim, train_sched = net.custom_compile()\n","    train_logs = []\n","    val_logs_1, val_logs_2 = [], []\n","    test_logs_1, test_logs_2 = [], []\n","    for epoch in range(epochs):\n","      permutation = utils.shuffle(train)   \n","      for step in range(steps):\n","        batch = utils.load_batch(step, train, batch_size, permutation, shuffle_epoch=False)\n","        train_log = net.train_step(model=net, batch=batch, optim=train_optim,\n","                                     scheduler=train_sched, train_logs=train_logs, batch_size=batch_size)\n","        train_logs.append(train_log)\n","      val_logs = net.evaluation_step(model=net,test_tuple=val, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      test_logs = net.evaluation_step(model=net,test_tuple=test, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      val_logs_1.append(val_logs[0]), val_logs_2.append(val_logs[1])      \n","      test_logs_1.append(test_logs[0]), test_logs_2.append(test_logs[1])\n","    experiment_logs.append([test_logs_1[np.argmin(val_logs_1)], test_logs_2[np.argmin(val_logs_1)]])\n","    utils.log_plots(model_name=net.custom_name, dataset_name=dataset, \n","                    train_logs = train_logs, val_logs_1 = val_logs_1,\n","                    val_logs_2 = val_logs_2, test_logs_1 = test_logs_1,\n","                     test_logs_2 = test_logs_2, remove_n =0, log_transform=False)       \n","    \n","  final_logs.append(experiment_logs)\n","results.resultdf(model_list, final_logs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-kAL-2YnY-tr"},"source":["batch_size = 2048*3\n","dataset, label_index = 'naval-propulsion-plant', 16\n","importlib.reload(models)\n","final_logs = []\n","best_log = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for experiment in range(n_experiments):\n","  experiment_logs = []\n","  seed = experiment\n","  train, val, test, target_scale = pipelines.from_git(dataset=dataset, \n","                                  label_index=label_index,\n","                                  split=train_split, seed=seed, device=device)\n","  n_features=train[0].shape[1]\n","  n_train = train[0].shape[0]\n","  steps = np.int(n_train/batch_size)+1\n","  target_scale = target_scale\n","  print(f\"Experiment {experiment+1}: train sample: {n_train}, batchsize: {batch_size}, steps: {steps}\")\n","  model_list = [models.Baseline, models.MCDropout, models.SDENet, models.IVNet]   \n","  for model in model_list:\n","    net = model(n_features=n_features, n_hidden=n_hidden, tau=tau, n_train=n_train, layer_depth=6, device=device).to(device)\n","    train_optim, train_sched = net.custom_compile()\n","    train_logs = []\n","    val_logs_1, val_logs_2 = [], []\n","    test_logs_1, test_logs_2 = [], []\n","    for epoch in range(epochs):\n","      permutation = utils.shuffle(train)   \n","      for step in range(steps):\n","        batch = utils.load_batch(step, train, batch_size, permutation, shuffle_epoch=False)\n","        train_log = net.train_step(model=net, batch=batch, optim=train_optim,\n","                                     scheduler=train_sched, train_logs=train_logs, batch_size=batch_size)\n","        train_logs.append(train_log)\n","      val_logs = net.evaluation_step(model=net,test_tuple=val, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      test_logs = net.evaluation_step(model=net,test_tuple=test, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      val_logs_1.append(val_logs[0]), val_logs_2.append(val_logs[1])      \n","      test_logs_1.append(test_logs[0]), test_logs_2.append(test_logs[1])\n","    experiment_logs.append([test_logs_1[np.argmin(val_logs_2)], test_logs_2[np.argmin(val_logs_2)]])\n","    utils.log_plots(model_name=net.custom_name, dataset_name=dataset, \n","                    train_logs = train_logs, val_logs_1 = val_logs_1,\n","                    val_logs_2 = val_logs_2, test_logs_1 = test_logs_1,\n","                     test_logs_2 = test_logs_2, remove_n =0, log_transform=True)       \n","    \n","  final_logs.append(experiment_logs)\n","results.resultdf(model_list, final_logs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVOmF4oCZDK2"},"source":["batch_size = 512\n","dataset, label_index = 'concrete', 8\n","importlib.reload(models)\n","final_logs = []\n","best_log = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for experiment in range(n_experiments):\n","  experiment_logs = []\n","  seed = experiment\n","  train, val, test, target_scale = pipelines.from_git(dataset=dataset, \n","                                  label_index=label_index,\n","                                  split=train_split, seed=seed, device=device)\n","  n_features=train[0].shape[1]\n","  n_train = train[0].shape[0]\n","  steps = np.int(n_train/batch_size)+1\n","  target_scale = target_scale\n","  print(f\"Experiment {experiment+1}: train sample: {n_train}, batchsize: {batch_size}, steps: {steps}\")\n","  model_list = [models.Baseline, models.MCDropout, models.SDENet, models.IVNet]   \n","  for model in model_list:\n","    net = model(n_features=n_features, n_hidden=n_hidden, tau=tau, n_train=n_train, layer_depth=6, device=device).to(device)\n","    train_optim, train_sched = net.custom_compile()\n","    train_logs = []\n","    val_logs_1, val_logs_2 = [], []\n","    test_logs_1, test_logs_2 = [], []\n","    for epoch in range(epochs):\n","      permutation = utils.shuffle(train)   \n","      for step in range(steps):\n","        batch = utils.load_batch(step, train, batch_size, permutation, shuffle_epoch=False)\n","        train_log = net.train_step(model=net, batch=batch, optim=train_optim,\n","                                     scheduler=train_sched, train_logs=train_logs, batch_size=batch_size)\n","        train_logs.append(train_log)\n","      val_logs = net.evaluation_step(model=net,test_tuple=val, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      test_logs = net.evaluation_step(model=net,test_tuple=test, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      val_logs_1.append(val_logs[0]), val_logs_2.append(val_logs[1])      \n","      test_logs_1.append(test_logs[0]), test_logs_2.append(test_logs[1])\n","    experiment_logs.append([test_logs_1[np.argmin(val_logs_2)], test_logs_2[np.argmin(val_logs_2)]])\n","    utils.log_plots(model_name=net.custom_name, dataset_name=dataset, \n","                    train_logs = train_logs, val_logs_1 = val_logs_1,\n","                    val_logs_2 = val_logs_2, test_logs_1 = test_logs_1,\n","                     test_logs_2 = test_logs_2, remove_n =0, log_transform=True)       \n","    \n","  final_logs.append(experiment_logs)\n","results.resultdf(model_list, final_logs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ysaFifMKEH2Y"},"source":["batch_size = 256\n","dataset, label_index = 'energy', 8\n","importlib.reload(models)\n","final_logs = []\n","best_log = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for experiment in range(n_experiments):\n","  experiment_logs = []\n","  seed = experiment\n","  train, val, test, target_scale = pipelines.from_git(dataset=dataset, \n","                                  label_index=label_index,\n","                                  split=train_split, seed=seed, device=device)\n","  n_features=train[0].shape[1]\n","  n_train = train[0].shape[0]\n","  steps = np.int(n_train/batch_size)+1\n","  target_scale = target_scale\n","  print(f\"Experiment {experiment+1}: train sample: {n_train}, batchsize: {batch_size}, steps: {steps}\")\n","  model_list = [models.Baseline, models.MCDropout, models.SDENet, models.IVNet]   \n","  for model in model_list:\n","    net = model(n_features=n_features, n_hidden=n_hidden, tau=tau, n_train=n_train, layer_depth=6, device=device).to(device)\n","    train_optim, train_sched = net.custom_compile()\n","    train_logs = []\n","    val_logs_1, val_logs_2 = [], []\n","    test_logs_1, test_logs_2 = [], []\n","    for epoch in range(epochs):\n","      permutation = utils.shuffle(train)   \n","      for step in range(steps):\n","        batch = utils.load_batch(step, train, batch_size, permutation, shuffle_epoch=False)\n","        train_log = net.train_step(model=net, batch=batch, optim=train_optim,\n","                                     scheduler=train_sched, train_logs=train_logs, batch_size=batch_size)\n","        train_logs.append(train_log)\n","      val_logs = net.evaluation_step(model=net,test_tuple=val, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      test_logs = net.evaluation_step(model=net,test_tuple=test, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      val_logs_1.append(val_logs[0]), val_logs_2.append(val_logs[1])      \n","      test_logs_1.append(test_logs[0]), test_logs_2.append(test_logs[1])\n","    experiment_logs.append([test_logs_1[np.argmin(val_logs_2)], test_logs_2[np.argmin(val_logs_2)]])\n","    utils.log_plots(model_name=net.custom_name, dataset_name=dataset, \n","                    train_logs = train_logs, val_logs_1 = val_logs_1,\n","                    val_logs_2 = val_logs_2, test_logs_1 = test_logs_1,\n","                     test_logs_2 = test_logs_2, remove_n =0, log_transform=True)       \n","    \n","  final_logs.append(experiment_logs)\n","results.resultdf(model_list, final_logs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVfkjOUt_qyg"},"source":["batch_size = 2048*2\n","dataset, label_index = 'kin8nm', 8\n","importlib.reload(models)\n","final_logs = []\n","best_log = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for experiment in range(n_experiments):\n","  experiment_logs = []\n","  seed = experiment\n","  train, val, test, target_scale = pipelines.from_git(dataset=dataset, \n","                                  label_index=label_index,\n","                                  split=train_split, seed=seed, device=device)\n","  n_features=train[0].shape[1]\n","  n_train = train[0].shape[0]\n","  steps = np.int(n_train/batch_size)+1\n","  target_scale = target_scale\n","  print(f\"Experiment {experiment+1}: train sample: {n_train}, batchsize: {batch_size}, steps: {steps}\")\n","  model_list = [models.Baseline, models.MCDropout, models.SDENet, models.IVNet]   \n","  for model in model_list:\n","    net = model(n_features=n_features, n_hidden=n_hidden, tau=tau, n_train=n_train, layer_depth=6, device=device).to(device)\n","    train_optim, train_sched = net.custom_compile()\n","    train_logs = []\n","    val_logs_1, val_logs_2 = [], []\n","    test_logs_1, test_logs_2 = [], []\n","    for epoch in range(epochs):\n","      permutation = utils.shuffle(train)   \n","      for step in range(steps):\n","        batch = utils.load_batch(step, train, batch_size, permutation, shuffle_epoch=False)\n","        train_log = net.train_step(model=net, batch=batch, optim=train_optim,\n","                                     scheduler=train_sched, train_logs=train_logs, batch_size=batch_size)\n","        train_logs.append(train_log)\n","      val_logs = net.evaluation_step(model=net,test_tuple=val, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      test_logs = net.evaluation_step(model=net,test_tuple=test, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      val_logs_1.append(val_logs[0]), val_logs_2.append(val_logs[1])      \n","      test_logs_1.append(test_logs[0]), test_logs_2.append(test_logs[1])\n","    experiment_logs.append([test_logs_1[np.argmin(val_logs_2)], test_logs_2[np.argmin(val_logs_2)]])\n","    utils.log_plots(model_name=net.custom_name, dataset_name=dataset, \n","                    train_logs = train_logs, val_logs_1 = val_logs_1,\n","                    val_logs_2 = val_logs_2, test_logs_1 = test_logs_1,\n","                     test_logs_2 = test_logs_2, remove_n =0, log_transform=True)       \n","    \n","  final_logs.append(experiment_logs)\n","results.resultdf(model_list, final_logs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwtIqCjCERwT"},"source":["batch_size = 2048*2\n","dataset, label_index = 'power-plant', 4\n","importlib.reload(models)\n","final_logs = []\n","best_log = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for experiment in range(n_experiments):\n","  experiment_logs = []\n","  seed = experiment\n","  train, val, test, target_scale = pipelines.from_git(dataset=dataset, \n","                                  label_index=label_index,\n","                                  split=train_split, seed=seed, device=device)\n","  n_features=train[0].shape[1]\n","  n_train = train[0].shape[0]\n","  steps = np.int(n_train/batch_size)+1\n","  target_scale = target_scale\n","  print(f\"Experiment {experiment+1}: train sample: {n_train}, batchsize: {batch_size}, steps: {steps}\")\n","  model_list = [models.Baseline, models.MCDropout, models.SDENet, models.IVNet]   \n","  for model in model_list:\n","    net = model(n_features=n_features, n_hidden=n_hidden, tau=tau, n_train=n_train, layer_depth=6, device=device).to(device)\n","    train_optim, train_sched = net.custom_compile()\n","    train_logs = []\n","    val_logs_1, val_logs_2 = [], []\n","    test_logs_1, test_logs_2 = [], []\n","    for epoch in range(epochs):\n","      permutation = utils.shuffle(train)   \n","      for step in range(steps):\n","        batch = utils.load_batch(step, train, batch_size, permutation, shuffle_epoch=False)\n","        train_log = net.train_step(model=net, batch=batch, optim=train_optim,\n","                                     scheduler=train_sched, train_logs=train_logs, batch_size=batch_size)\n","        train_logs.append(train_log)\n","      val_logs = net.evaluation_step(model=net,test_tuple=val, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      test_logs = net.evaluation_step(model=net,test_tuple=test, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      val_logs_1.append(val_logs[0]), val_logs_2.append(val_logs[1])      \n","      test_logs_1.append(test_logs[0]), test_logs_2.append(test_logs[1])\n","    experiment_logs.append([test_logs_1[np.argmin(val_logs_2)], test_logs_2[np.argmin(val_logs_2)]])\n","    utils.log_plots(model_name=net.custom_name, dataset_name=dataset, \n","                    train_logs = train_logs, val_logs_1 = val_logs_1,\n","                    val_logs_2 = val_logs_2, test_logs_1 = test_logs_1,\n","                     test_logs_2 = test_logs_2, remove_n =0, log_transform=True)       \n","    \n","  final_logs.append(experiment_logs)\n","results.resultdf(model_list, final_logs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-E9IygSEb0g"},"source":["batch_size = 2048*2\n","dataset, label_index = 'protein-tertiary-structure', 9\n","importlib.reload(models)\n","final_logs = []\n","best_log = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for experiment in range(n_experiments):\n","  experiment_logs = []\n","  seed = experiment\n","  train, val, test, target_scale = pipelines.from_git(dataset=dataset, \n","                                  label_index=label_index,\n","                                  split=train_split, seed=seed, device=device)\n","  n_features=train[0].shape[1]\n","  n_train = train[0].shape[0]\n","  steps = np.int(n_train/batch_size)+1\n","  target_scale = target_scale\n","  print(f\"Experiment {experiment+1}: train sample: {n_train}, batchsize: {batch_size}, steps: {steps}\")\n","  model_list = [models.Baseline, models.MCDropout, models.SDENet, models.IVNet]   \n","  for model in model_list:\n","    net = model(n_features=n_features, n_hidden=n_hidden, tau=tau, n_train=n_train, layer_depth=6, device=device).to(device)\n","    train_optim, train_sched = net.custom_compile()\n","    train_logs = []\n","    val_logs_1, val_logs_2 = [], []\n","    test_logs_1, test_logs_2 = [], []\n","    for epoch in range(epochs):\n","      permutation = utils.shuffle(train)   \n","      for step in range(steps):\n","        batch = utils.load_batch(step, train, batch_size, permutation, shuffle_epoch=False)\n","        train_log = net.train_step(model=net, batch=batch, optim=train_optim,\n","                                     scheduler=train_sched, train_logs=train_logs, batch_size=batch_size)\n","        train_logs.append(train_log)\n","      val_logs = net.evaluation_step(model=net,test_tuple=val, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      test_logs = net.evaluation_step(model=net,test_tuple=test, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      val_logs_1.append(val_logs[0]), val_logs_2.append(val_logs[1])      \n","      test_logs_1.append(test_logs[0]), test_logs_2.append(test_logs[1])\n","    experiment_logs.append([test_logs_1[np.argmin(val_logs_2)], test_logs_2[np.argmin(val_logs_2)]])\n","    utils.log_plots(model_name=net.custom_name, dataset_name=dataset, \n","                    train_logs = train_logs, val_logs_1 = val_logs_1,\n","                    val_logs_2 = val_logs_2, test_logs_1 = test_logs_1,\n","                     test_logs_2 = test_logs_2, remove_n =0, log_transform=True)       \n","    \n","  final_logs.append(experiment_logs)\n","results.resultdf(model_list, final_logs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xivkDAutEh5O"},"source":["batch_size = 128\n","dataset, label_index = 'yacht', 6\n","importlib.reload(models)\n","final_logs = []\n","best_log = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","for experiment in range(n_experiments):\n","  experiment_logs = []\n","  seed = experiment\n","  train, val, test, target_scale = pipelines.from_git(dataset=dataset, \n","                                  label_index=label_index,\n","                                  split=train_split, seed=seed, device=device)\n","  n_features=train[0].shape[1]\n","  n_train = train[0].shape[0]\n","  steps = np.int(n_train/batch_size)+1\n","  target_scale = target_scale\n","  print(f\"Experiment {experiment+1}: train sample: {n_train}, batchsize: {batch_size}, steps: {steps}\")\n","  model_list = [models.Baseline, models.MCDropout, models.SDENet, models.IVNet]   \n","  for model in model_list:\n","    net = model(n_features=n_features, n_hidden=n_hidden, tau=tau, n_train=n_train, layer_depth=6, device=device).to(device)\n","    train_optim, train_sched = net.custom_compile()\n","    train_logs = []\n","    val_logs_1, val_logs_2 = [], []\n","    test_logs_1, test_logs_2 = [], []\n","    for epoch in range(epochs):\n","      permutation = utils.shuffle(train)   \n","      for step in range(steps):\n","        batch = utils.load_batch(step, train, batch_size, permutation, shuffle_epoch=False)\n","        train_log = net.train_step(model=net, batch=batch, optim=train_optim,\n","                                     scheduler=train_sched, train_logs=train_logs, batch_size=batch_size)\n","        train_logs.append(train_log)\n","      val_logs = net.evaluation_step(model=net,test_tuple=val, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      test_logs = net.evaluation_step(model=net,test_tuple=test, eval_func=utils.evaluation,\n","                                       target_scale=target_scale, test_iters=test_iters)\n","      val_logs_1.append(val_logs[0]), val_logs_2.append(val_logs[1])      \n","      test_logs_1.append(test_logs[0]), test_logs_2.append(test_logs[1])\n","    experiment_logs.append([test_logs_1[np.argmin(val_logs_2)], test_logs_2[np.argmin(val_logs_2)]])\n","    utils.log_plots(model_name=net.custom_name, dataset_name=dataset, \n","                    train_logs = train_logs, val_logs_1 = val_logs_1,\n","                    val_logs_2 = val_logs_2, test_logs_1 = test_logs_1,\n","                     test_logs_2 = test_logs_2, remove_n =0, log_transform=True)       \n","    \n","  final_logs.append(experiment_logs)\n","results.resultdf(model_list, final_logs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35GdKehQEtFh"},"source":["# variable-annuity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMVlrHTjZlAz"},"source":["# claims frequency modelling\n","\n","url = '/content/drive/My Drive/Research/Data/IP/FrenchMotor/Freq.csv'\n","\n","data = pd.read_csv(url)\n","\n","# remove rows with ClaimNb > 4 and Exposure >1\n","\n","X_raw = data.drop(data.columns[[0,1,2,3]], axis=1)\n","\n","# target = data['ClaimNb]/data['Exposure]\n","y_raw = data['ClaimNb']\n","y_raw = np.exp(np.array(y_df, dtype=np.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5C0RkEzrZvPp"},"source":["data.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vv3rablGkYxL"},"source":["np.exp(np.array(y_df, dtype=np.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DTlTl_V1uFp1"},"source":["X_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"govSb0_4vPVX"},"source":[""],"execution_count":null,"outputs":[]}]}